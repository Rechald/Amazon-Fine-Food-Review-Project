{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ziyao Chen hw4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/rstudio/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import urllib\n",
    "import scipy.optimize\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import string\n",
    "import math\n",
    "from nltk.stem.porter import *\n",
    "from sklearn import linear_model\n",
    "nltk.download('punkt')\n",
    "from collections import Counter\n",
    "def parseData(fname):\n",
    "    for l in urllib.request.urlopen(fname):\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "### Just the first 5000 reviews\n",
    "\n",
    "print (\"Reading data\")\n",
    "data = list(parseData(\"http://jmcauley.ucsd.edu/cse158/data/beer/beer_50000.json\"))[:5000]\n",
    "print (\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'review/appearance': 2.5,\n",
       "  'beer/style': 'Hefeweizen',\n",
       "  'review/palate': 1.5,\n",
       "  'review/taste': 1.5,\n",
       "  'beer/name': 'Sausa Weizen',\n",
       "  'review/timeUnix': 1234817823,\n",
       "  'beer/ABV': 5.0,\n",
       "  'beer/beerId': '47986',\n",
       "  'beer/brewerId': '10325',\n",
       "  'review/timeStruct': {'isdst': 0,\n",
       "   'mday': 16,\n",
       "   'hour': 20,\n",
       "   'min': 57,\n",
       "   'sec': 3,\n",
       "   'mon': 2,\n",
       "   'year': 2009,\n",
       "   'yday': 47,\n",
       "   'wday': 0},\n",
       "  'review/overall': 1.5,\n",
       "  'review/text': 'A lot of foam. But a lot.\\tIn the smell some banana, and then lactic and tart. Not a good start.\\tQuite dark orange in color, with a lively carbonation (now visible, under the foam).\\tAgain tending to lactic sourness.\\tSame for the taste. With some yeast and banana.',\n",
       "  'user/profileName': 'stcules',\n",
       "  'review/aroma': 2.0}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1.How many unique bigrams are there amongst all of the reviews? List the 5 most-frequently-occurring\n",
    "bigrams along with their number of occurrences in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  182218  unique bigrams.\n"
     ]
    }
   ],
   "source": [
    "#read the reviews without capitalization or punctuation\n",
    "### Ignore capitalization and remove punctuation\n",
    "\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "stemmer = PorterStemmer()\n",
    "for d in data:\n",
    "    r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])\n",
    "    r_token=nltk.word_tokenize(r)\n",
    "    r_bigrams=list(nltk.bigrams(r_token))\n",
    "    for w in r_bigrams:\n",
    "    #w = stemmer.stem(w) # with stemming\n",
    "        wordCount[w] += 1\n",
    "print (\"There are \",len(wordCount),\" unique bigrams.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Just take the most popular words...\n",
    "\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "for d in data:\n",
    "    r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])\n",
    "    r_token=nltk.word_tokenize(r)\n",
    "    r_bigrams=list(nltk.bigrams(r_token))\n",
    "    for w in r_bigrams:\n",
    "        wordCount[w] += 1\n",
    "\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "\n",
    "words1 = [x[1] for x in counts[:1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4587, ('with', 'a')),\n",
       " (2595, ('in', 'the')),\n",
       " (2245, ('of', 'the')),\n",
       " (2056, ('is', 'a')),\n",
       " (2033, ('on', 'the'))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There are 36225 unique bigrams. The 5 most-frequently-occurring bigrams along with their number of occurrences in the corpus are (30695, 'a'), (27569, 'the'), (19512, 'and'), (15935, 'of'), (12623, 'is')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.The code provided performs least squares using the 1000 most common unigrams. Adapt it to use\n",
    "the 1000 most common bigrams and report the MSE obtained using the new predictor (use bigrams\n",
    "only, i.e., not unigrams+bigrams) (1 mark). Note that the code performs regularized regression with a\n",
    "regularization parameter of 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "for d in data:\n",
    "    r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])\n",
    "    r_token=nltk.word_tokenize(r)\n",
    "    r_bigrams=list(nltk.bigrams(r_token))\n",
    "    for w in r_bigrams:\n",
    "        wordCount[w] += 1\n",
    "\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "\n",
    "words = [x[1] for x in counts[:1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import numpy\n",
    "#len(numpy.unique(words))\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sentiment analysis\n",
    "\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)\n",
    "\n",
    "def feature(datum):\n",
    "    feat = [0]*len(words)\n",
    "    r = ''.join([c for c in datum['review/text'].lower() if not c in punctuation])\n",
    "    r_token=nltk.word_tokenize(r)\n",
    "    r_bigrams=list(nltk.bigrams(r_token))\n",
    "    for w in r_bigrams:\n",
    "        if w in words:\n",
    "            feat[wordId[w]] += 1\n",
    "    feat.append(1) #offset\n",
    "    return feat\n",
    "\n",
    "X = [feature(d) for d in data]\n",
    "y = [d['review/overall'] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With regularization\n",
    "clf = linear_model.Ridge(1.0, fit_intercept=False)\n",
    "clf.fit(X, y)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3432257072347801"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate MSE\n",
    "zipped = [a - b for a, b in zip(numpy.array(y),numpy.array(predictions))]\n",
    "myarray = numpy.asarray(zipped)\n",
    "mse = numpy.mean(myarray**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3432257072347801"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The MSE obtained using the new predictor is 0.3432257072347801."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3.What is the inverse document frequency of the words `foam', `smell', `banana', `lactic', and `tart'? What are their tf-idf scores in the \f",
    "rst review (using log base 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "document=[]\n",
    "for d in data:\n",
    "    r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])\n",
    "    document.append(r.split())\n",
    "    #document.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(wordlist,document):\n",
    "    idf_cal= defaultdict(set)\n",
    "    idfDict= defaultdict(set)\n",
    "    N=len(document)\n",
    "\n",
    "    for word in wordlist:\n",
    "        i=0\n",
    "        for d in document:\n",
    "            if word in d:\n",
    "                i=i+1\n",
    "        idf_cal[word]=i\n",
    "                \n",
    "    for word, value in idf_cal.items():\n",
    "        idfDict[word]=math.log10(N/float(value))\n",
    "        \n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist=['foam','smell','banana','lactic','tart']\n",
    "idf_q3=computeIDF(wordlist,document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The inverse document frequency of the words are shown as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {'foam': 1.1378686206869628,\n",
       "             'smell': 0.5379016188648442,\n",
       "             'banana': 1.6777807052660807,\n",
       "             'lactic': 2.9208187539523753,\n",
       "             'tart': 1.8068754016455384})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate tf for the first review\n",
    "def computeTF(wordlist,document):\n",
    "    tfDict= defaultdict(set)\n",
    "    \n",
    "    for word in wordlist:\n",
    "        count=0\n",
    "        for d in document:\n",
    "                if word == d:\n",
    "                    count+=1\n",
    "        tfDict[word]=count\n",
    "        \n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "document0=document[0]\n",
    "tf_r1=computeTF(wordlist,document0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set, {'foam': 2, 'smell': 1, 'banana': 2, 'lactic': 2, 'tart': 1})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate tf-idf \n",
    "def computeTFIDF(wordlist,tf,idf):\n",
    "    tfidf=defaultdict(set)\n",
    "    for word in wordlist:\n",
    "        tfidf[word]=tf[word]*idf[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {'foam': 2.2757372413739256,\n",
       "             'smell': 0.5379016188648442,\n",
       "             'banana': 3.3555614105321614,\n",
       "             'lactic': 5.841637507904751,\n",
       "             'tart': 1.8068754016455384})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_r1=computeTFIDF(wordlist,tf_r1,idf_q3)\n",
    "tfidf_r1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The tfidf scores are shown as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4.What is the cosine similarity between the first and the second review in terms of their tf-idf representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to calculat tf-idf\n",
    "def computeTF_IDF(wordlist,documentset):\n",
    "    tfDict= defaultdict(set)\n",
    "    idf_cal= defaultdict(set)\n",
    "    idfDict= defaultdict(set)\n",
    "    tfidf=defaultdict(set)\n",
    "    N=len(document)\n",
    "    \n",
    "    # tf\n",
    "    for word in wordlist:\n",
    "        count=0\n",
    "        for d in documentset:\n",
    "                if word == d:\n",
    "                    count+=1\n",
    "        tfDict[word]=count\n",
    "        \n",
    "    #idf    \n",
    "        i=0\n",
    "        for d in document:\n",
    "            if word in d:\n",
    "                i=i+1\n",
    "        idf_cal[word]=i\n",
    "        \n",
    "    for word, value in idf_cal.items():\n",
    "        idfDict[word]=math.log10(N/float(value))   \n",
    "    \n",
    "    #tf-idf\n",
    "    for word in wordlist:\n",
    "        tfidf[word]=tfDict[word]*idfDict[word]\n",
    "    \n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find common words between the first and second review\n",
    "r1=document[0]\n",
    "r2=document[1]\n",
    "wordlist_q4 = set(r1)&set(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(wordlist_q4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the tf-idf score of reviews\n",
    "tfidf_q4_r1=computeTF_IDF(wordlist_q4,r1)\n",
    "tfidf_q4_r2=computeTF_IDF(wordlist_q4,r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {'foam': 2.2757372413739256,\n",
       "             'smell': 0.5379016188648442,\n",
       "             'in': 0.3496672554869006,\n",
       "             'dark': 0.5512936800949202,\n",
       "             'taste': 0.2914091548496563,\n",
       "             'not': 0.2823294969977378,\n",
       "             'the': 0.0867293605886536,\n",
       "             'color': 0.46067293646062496,\n",
       "             'with': 0.12436263060328803,\n",
       "             'of': 0.05158703422139899,\n",
       "             'but': 0.1663430031071276,\n",
       "             'again': 0.8787685448503786,\n",
       "             'a': 0.024492340235155226,\n",
       "             'and': 0.0979177801021508})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_q4_r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn set in to lists\n",
    "list1=[]\n",
    "list2=[]\n",
    "for key,value in tfidf_q4_r1.items():\n",
    "    list1.append(value)\n",
    "    \n",
    "for key,value in tfidf_q4_r2.items():\n",
    "    list2.append(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " calculate cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity([list1], [list2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf1=computeTF_IDF(r1,r1)\n",
    "sum1=sum(tfidf1[x]**2 for x in tfidf1.keys())\n",
    "tfidf2=computeTF_IDF(r2,r2)\n",
    "sum2=sum(tfidf2[x]**2 for x in tfidf2.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "sim=0\n",
    "for i in range(len(list1)):\n",
    "    sim+=list1[i]*list2[i]/(math.sqrt(sum1)*math.sqrt(sum2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06588193974744379"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The cosine similarity between the first and the second review is 0.0658819397474438."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5.Which other review has the highest cosine similarity compared to the \f",
    "rst review (provide the beerId\n",
    "and pro\f",
    "leName, or the text of the review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a idf dic\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "for d in data:\n",
    "  r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])\n",
    "  for w in r.split():\n",
    "    wordCount[w] += 1\n",
    "\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "words = [x[1] for x in counts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_q5=computeIDF(words,document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to calculat tf-idf\n",
    "def computeTF_IDF5(wordlist,documentset):\n",
    "    tfDict= defaultdict(set)\n",
    "    tfidf=defaultdict(set)\n",
    "    \n",
    "    # tf\n",
    "    for word in wordlist:\n",
    "        count=0\n",
    "        for d in documentset:\n",
    "                if word == d:\n",
    "                    count+=1\n",
    "        tfDict[word]=count\n",
    "         \n",
    "        tfidf[word]=tfDict[word]*idf_q5[word]\n",
    "    \n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentget=defaultdict(set)\n",
    "n=0\n",
    "for d in document:\n",
    "    documentget[n]=d\n",
    "    n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosdic=[]\n",
    "for i in range(1,5001):\n",
    "    #calculate tf-idf\n",
    "    review=documentget[i]\n",
    "    wordlist_q5 = set(r1)&set(review)\n",
    "    tfidf_q5_review=computeTF_IDF5(wordlist_q5,review)\n",
    "    tfidf_q5_r1=computeTF_IDF5(wordlist_q5,r1)\n",
    "    \n",
    "    #calculate cosine\n",
    "    tfidfb=computeTF_IDF5(review,review)\n",
    "    sum2=sum(tfidfb[x]**2 for x in tfidfb.keys())\n",
    "    if sum2==0:\n",
    "        cosdic.append(0)\n",
    "    else:\n",
    "        cosine_similarity=sum([tfidf_q5_review[x]*tfidf_q5_r1[x] for x in wordlist_q5])/(math.sqrt(sum1)*math.sqrt(sum2))\n",
    "        cosdic.append(cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2342"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosdic.index(max(cosdic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2968679537499198"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(cosdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review/appearance': 4.0,\n",
       " 'beer/style': 'Saison / Farmhouse Ale',\n",
       " 'review/palate': 4.0,\n",
       " 'review/taste': 3.5,\n",
       " 'beer/name': 'Her Majesty 2011',\n",
       " 'review/timeUnix': 1317979248,\n",
       " 'user/gender': 'Male',\n",
       " 'user/birthdayRaw': 'Mar 14, 1977',\n",
       " 'beer/ABV': 8.0,\n",
       " 'beer/beerId': '72146',\n",
       " 'user/birthdayUnix': 227174400,\n",
       " 'beer/brewerId': '23246',\n",
       " 'review/timeStruct': {'isdst': 0,\n",
       "  'mday': 7,\n",
       "  'hour': 9,\n",
       "  'min': 20,\n",
       "  'sec': 48,\n",
       "  'mon': 10,\n",
       "  'year': 2011,\n",
       "  'yday': 280,\n",
       "  'wday': 4},\n",
       " 'user/ageInSeconds': 1191161047,\n",
       " 'review/overall': 4.0,\n",
       " 'review/text': \"750mL bottle thanks to Chris@Slowbeer. Poured into a Lost Abbey stemmed tulip.\\t\\tGolden orange, close to translucent (on the first pour at least), capped by a sizable white, typically Belgian-looking head. Good lacing.\\t\\tQuite strong lactic notes and a sharp organic funk. Pungent stuff. Underneath is bitter citrus pith, floral spice and a hint of sweet esters. In your face with a lot going on. Only issue is the lactic character verges on turning my stomach.\\t\\tMore citric sourness and a bit less lactic character. Grapefruit and lemon rind are prominent, as is the Nelson Sauvin vegetative character, which kind of adheres to the yeast and barnyard funk. Tropical melons and honey provide some sweetness. Decent peppery tang.\\t\\tMedium, lightly syrupy body with lowish carbonation and a moderately tart, dry finish that has some length to it.\\t\\tIncomparable to anything I've tried. The Sauvin hops with the Saison yeast is a masterful combination, however there's no shortage of rough edges, which prevents an amazing or highly drinkable result.\",\n",
       " 'user/profileName': 'spicelab',\n",
       " 'review/aroma': 3.5}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[2343]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 'beer/beerId': '72146' has the highest cosine similarity compared to the first review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6.Adapt the original model that uses the 1000 most common unigrams, but replace the features with their\n",
    "1000-dimensional tf-idf representations, and report the MSE obtained with the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "for d in data:\n",
    "  r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])\n",
    "  for w in r.split():\n",
    "    wordCount[w] += 1\n",
    "\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "\n",
    "words1 = [x[1] for x in counts[:1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [computeTF_IDF5(words1,d) for d in document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methond to convert dict to list\n",
    "final=[]\n",
    "for i in range(0,5000):\n",
    "    x_try=[]\n",
    "    for y in X[i]:\n",
    "        x_try.append(X[i][y])\n",
    "    final.append(x_try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.5, 3.0, 3.0, 3.0, 4.0, 3.0, 3.5, 3.0, 4.0, 4.5]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [d['review/overall'] for d in data]\n",
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With regularization\n",
    "clf = linear_model.Ridge(1.0, fit_intercept=False)\n",
    "clf.fit(final, y)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1349391271538383"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
